# 方法一：未自定义Patitioner

## 基本逻辑

#### Mapper

* Input
  * key：文本段落相对于全文的line offset
  * value：第offset行的文本内容
  
* Output
  * key：有效单词
  * value：文件名#词频
  
* 操作流程：
  
  * 读入停用词表：**停用词表只在Map过程用到**
  
    * 重构Mapper的setup函数，将停用词表读入；
  
      > * setup函数的作用：
      >   * MapReduce框架仅执行一次，在执行Map任务前，进行相关变量或者资源的集中初始化工作；
      >   * 若是将资源初始化工作放在方法map()中，导致Mapper任务在解析每一行输入时都会进行资源初始化工作，导致重复，程序运行效率不高；
      > * cleanup函数的作用：
      >   * MapReduce框架仅执行一次，在执行完毕Map任务后，进行相关变量或资源的释放工作；
      >   * 若是将释放资源工作放入方法map()中，也会导 致Mapper任务在解析、处理每一行文本后释放资源，而且在下一行文本解析前还要重复初始化，导致反复重复，程序运行效率不高；
  
  * 获取文件名
  * 根据正则表达式分割第offset行的文本，获得该行单词集
  * 使用Map结构，对单行重复的单词计数
  * 将Map中的内容输出，格式为：<word, fileName#wordcount>

#### Reducer

* Input

  * key：单词
  * value：文件名#词频

* Output

  * key：单词
  * value：<文件1, 词频>;<文件2, 词频>;...;<total, 总词频>;

* 操作流程：

  * 遍历Input的values，将相同文件名项目的词频数相加，整合为1个<filename, count>项目

    ==为了保证文件名有序，使用TreeMap实现这个过程，key是文件名==

  * 遍历TreeMap的项目，拼接字符串得到**<文件名, 词频>;**样式的输出

  * 最后在尾部加上<total, 总词频>;得到最终输出

## 注意事项

* 读取停用词时需要使用trim去除词头尾空格；
* 使用HashSet或TreeSet保存stopwords，便于快速查找；
* 使用TreeMap合并相同文件名的词频，保证文件名有序；

## 问题

* 在Recude过程中进行<filename, count>集合的排序，需要将该集合暂存于内存，在数据较少时可以正常运行，但是数据庞大的情况下会出现内存不足的问题；

  ==解决方案：自定义Partitioner，使<filename, count>在map过程中完成排序==

# 方法二：自定义Partitioner

## 基本逻辑

#### Mapper

* Input
  * key：文本段落相对于全文的line offset
  * value：第offset行的文本内容
* Output
  * key：有效单词#文件名
  * value：单词在对应文件中该行的词频
* 操作流程：

  * 读入停用词表

  * 获取文件名
  * 根据正则表达式分割第offset行的文本，获得该行单词集
  * 使用Map结构，对单行重复的单词计数
  * 将Map中的内容输出，格式为：<word#filename, wordcount>

#### Combiner

> 将相同key的value相加，缩小需要partition的数据个数；

* 操作流程：遍历所有的value，将其值相加得到待输出的value，key值不变输出；

#### Partitioner

> 将map/combiner输出的结果通过hash分配到多个partition中，分配的过程保证（字典序）有序，partition的结果作为reduce的输入；

* Input
  * key：单词+文件名，word#filename
  * value：词频，wordcount
* 操作流程：将key中的word取出，将其作为key传入super.getPartitioner()，只是用key中的word作为partition的依据；

#### Reducer

* Reducer共享的成员变量

  * prevWord：前一个reduce的数据项单词；
  * prevCount：当前reduce过程中对应<单词，文件名>的词频；
  * prevFile：当前reduce的数据项文件名；
  * outString：当前组合得到的输出串；
  * total：当前reduce过程中对应单词在所有文件中的词频；

* Input

  * key：单词+文件名，word#filename
  * value：词频，wordcount

* Output

  * key：单词
  * value：<文件1, 词频>;<文件2, 词频>;...;<total, 总词频>;

* 操作流程：

  * 分割得到当前reduce数据项的单词和文件名；

  * 比较当前处理数据单词和上一个单词

    * 当前单词 ≠ 上一个单词，说明该词汇总结束，整理数据将其输出，重置相关变量；

    * 当前单词 = 上一个单词，判断当前处理文件名和上一个文件名

      * ###### 当前文件名 ≠ 上一个文件名，说明该文件的汇总结束，整理书橱附加到outString，重置相关变量；

      * 当前文件名 = 上一个文件名，不做处理；

    * 每次reduce过程都要汇总词频，将其加到prevCount和total中；

  * 修改cleanup函数，在cleanup中将最后一组数据汇总输出；

## 注意事项

* reduce过程中需要判断多种情况，其中单词和文件名分别有对应的数据汇总，需要注意变量的重置和更新；
* 添加了combiner过程，能够在partition之前缩小数据规模，加快速度，减少资源占用；
